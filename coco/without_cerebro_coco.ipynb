{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e593b84-ec8b-4503-ac82-5fa1448b9722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install --upgrade Pillow\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install -U \"ray[default]\"\n",
    "# !pip install -U \"ray[tune]\"\n",
    "# !pip install boto3\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176ba30f-d899-4593-9078-d310462c091c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from coco_proc.vocabulary import Vocabulary\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d847218",
   "metadata": {},
   "source": [
    "## Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48c457c-7e8b-4173-9444-621c72880646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/home/ubuntu/data/\"\n",
    "\n",
    "train_metadata_url = \"s3://cerebro-coco-dataset-ca/annotations/captions_val2017.json\"\n",
    "train_multimedia_url = \"s3://cerebro-coco-dataset-ca/coco/val2017\"\n",
    "train_annotations_path = ROOT_PATH + \"annotations/captions_val2017.json\"\n",
    "train_metadata_download_path = ROOT_PATH + \"train_metadata.csv\"\n",
    "train_download_path = ROOT_PATH + \"train/downloaded\"\n",
    "train_output_path = ROOT_PATH + \"train/post_etl\"\n",
    "\n",
    "valid_metadata_url = \"s3://cerebro-coco-dataset-ca/annotations/captions_val2017.json\"\n",
    "valid_multimedia_url = \"s3://cerebro-coco-dataset-ca/coco/val2017\"\n",
    "valid_annotations_path = ROOT_PATH + \"annotations/captions_val2017.json\"\n",
    "valid_metadata_download_path = ROOT_PATH + \"valid_metadata.csv\"\n",
    "valid_download_path = ROOT_PATH + \"valid/downloaded\"\n",
    "valid_output_path = ROOT_PATH + \"valid/post_etl\"\n",
    "\n",
    "test_metadata_url = \"s3://cerebro-coco-dataset-ca/annotations/captions_val2017.json\"\n",
    "test_multimedia_url = \"s3://cerebro-coco-dataset-ca/coco/val2017\"\n",
    "test_annotations_path = ROOT_PATH + \"annotations/captions_val2017.json\"\n",
    "test_metadata_download_path = ROOT_PATH + \"test_metadata.csv\"\n",
    "test_download_path = ROOT_PATH + \"test/downloaded\"\n",
    "test_output_path = ROOT_PATH + \"test/post_etl\"\n",
    "\n",
    "checkpoints_dir = ROOT_PATH + \"model_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2c8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_ID = \"AKIA3MKZP7D2JR67FQFD\"\n",
    "ACCESS_KEY = \"QFOsggpi44VYZzudZP1IXZgw4nKMPW9ZmGOcNXNH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d5b113-5fdd-43f1-ba91-35300f158c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path(ROOT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_download_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(valid_download_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_download_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(train_output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(valid_output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(checkpoints_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43f649",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Pre Processing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53696c02-8887-4069-bcaa-525213b029b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metadata(annotations_path, mode=\"train\"):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    largest_caption_len = -1\n",
    "    data_json = Nonen\n",
    "        'id': [],\n",
    "        'file_name': [],\n",
    "        'height': [],\n",
    "        'width': [],\n",
    "        'captions': [],\n",
    "        'date_captured': []\n",
    "    }\n",
    "\n",
    "    dataset_modified = {\n",
    "        'id': [],\n",
    "        'file_name': [],\n",
    "        'height': [],\n",
    "        'width': [],\n",
    "        'captions': [],\n",
    "        'date_captured': []\n",
    "    }\n",
    "\n",
    "    annotations = {}\n",
    "    annotations_list = data_json['annotations']\n",
    "    for i in annotations_list:\n",
    "        if not i[\"image_id\"] in annotations:\n",
    "            annotations[i[\"image_id\"]] = []\n",
    "        annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "\n",
    "    for i in range(len(data_json['images'])):\n",
    "        if mode == \"train\":\n",
    "            for caption in annotations[data_json[\"images\"][i]['id']]:\n",
    "                tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "                if len(tokens) > largest_caption_len:\n",
    "                    largest_caption_len = len(tokens)\n",
    "                dataset['id'].append(data_json[\"images\"][i]['id'])\n",
    "                dataset['file_name'].append(data_json[\"images\"][i]['file_name'])\n",
    "                dataset['height'].append(data_json[\"images\"][i]['height'])\n",
    "                dataset['width'].append(data_json[\"images\"][i]['width'])\n",
    "                dataset['captions'].append(caption)\n",
    "                dataset['date_captured'].append(data_json[\"images\"][i]['date_captured'])\n",
    "        else:\n",
    "            caption = annotations[data_json[\"images\"][i]['id']][0]\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            if len(tokens) > largest_caption_len:\n",
    "                largest_caption_len = len(tokens)\n",
    "            dataset['id'].append(data_json[\"images\"][i]['id'])\n",
    "            dataset['file_name'].append(data_json[\"images\"][i]['file_name'])\n",
    "            dataset['height'].append(data_json[\"images\"][i]['height'])\n",
    "            dataset['width'].append(data_json[\"images\"][i]['width'])\n",
    "            dataset['captions'].append(caption)\n",
    "            dataset['date_captured'].append(data_json[\"images\"][i]['date_captured'])\n",
    "\n",
    "    pd_df = pd.DataFrame(dataset)\n",
    "    return pd_df, largest_caption_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef376a1-2100-4cca-82af-769c46607c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def row_preprocessing_routine(mode, row, to_root_path, kwargs):\n",
    "    \"\"\"\n",
    "    Convert a given dataset row to tensor format (suitable for training)\n",
    "    Data processing is a data parallel map operation. So, the same row_preprocessing_routine() will\n",
    "    be called on every row of the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : str\n",
    "        train/valid/test\n",
    "    row : pandas.core.series.Series\n",
    "        metadata pandas dataframe row\n",
    "    to_root_path : str\n",
    "        Path where all multimedia files will be stored on a node\n",
    "    kwargs : dict\n",
    "        A dictionary containing any extra arguments required specific to a ML task\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    id : str\n",
    "         uniquely identifying this row\n",
    "    input_tensor : torch.Tensor\n",
    "        All the input features should be combined to form a single input tensor\n",
    "    output_tensor : torch.Tensor\n",
    "        All the output features should be combined to form a single output tensor\n",
    "\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    import torch\n",
    "        \n",
    "    max_caption_len = kwargs[\"max_caption_len\"]\n",
    "    vocab = kwargs[\"vocab\"]\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        # Convert image to tensor and pre-process using transform\n",
    "        img_transform = transforms.Compose([ \n",
    "            transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "            transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "            transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "            transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "    else:\n",
    "        img_transform = transforms.Compose([ \n",
    "            transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "            transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "            transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # reading input features and converting to tensor\n",
    "    input_image_path = to_root_path + \"/\" + str(row[\"file_name\"])\n",
    "    image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    image_tensor = img_transform(image)\n",
    "    \n",
    "    # reading output features and converting to tensor\n",
    "    output_caption = row[\"captions\"]\n",
    "    tokens = nltk.tokenize.word_tokenize(str(output_caption).lower())\n",
    "    caption = []\n",
    "    caption.append(vocab(vocab.start_word))\n",
    "    caption.extend([vocab(token) for token in tokens])\n",
    "    caption.append(vocab(vocab.end_word))\n",
    "    \n",
    "    # padding\n",
    "    nremaining = max_caption_len - len(tokens)\n",
    "    if nremaining > 0:\n",
    "        for i in range(nremaining):\n",
    "            caption.append(vocab(vocab.end_word))\n",
    "    \n",
    "    caption_tensor = torch.Tensor(caption).long()\n",
    "    \n",
    "    # unique identifier for the row\n",
    "    image_id = row[\"id\"]\n",
    "\n",
    "    return image_id, image_tensor, caption_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b64727-34d8-4e3f-87d1-add680f879d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cce268b-094b-46d9-8b8e-4a3a5dd03b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_metadata():\n",
    "    remote_urls = [\n",
    "        train_metadata_url,\n",
    "        valid_metadata_url,\n",
    "        test_metadata_url\n",
    "    ]\n",
    "    \n",
    "    for remote_url in remote_urls:\n",
    "#         shutil.copy(remote_url, ROOT_PATH)\n",
    "        s3 = boto3.client('s3', aws_access_key_id=ACCESS_ID, aws_secret_access_key= ACCESS_KEY)\n",
    "        bucket_name = remote_url.split(\"/\")[2]\n",
    "        from_path = \"/\".join(remote_url.split(\"/\")[3:])\n",
    "        to_path = os.path.join(ROOT_PATH, from_path)\n",
    "\n",
    "        # create the to_path directory if it doesn't exist\n",
    "        to_path_dir = \"/\".join(to_path.split(\"/\")[:-1])\n",
    "        Path(to_path_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        s3.download_file(bucket_name, from_path, to_path)\n",
    "\n",
    "        print(\"Downloaded metadata\")\n",
    "#         etl_logger.info(\"Downloaded {} metadata\".format(mode if mode else \"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8e9a304-49fd-4837-8eb2-dc1da602b16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(metadata_file_path,fraction=1):\n",
    "        pandas_df = pd.read_csv(metadata_file_path)\n",
    "        metadata_df = pandas_df.sample(frac=fraction)\n",
    "        shuffled_df = metadata_df.sample(frac=1)\n",
    "        print(\"Loaded {} metadata to Pandas\".format(metadata_file_path))\n",
    "        return shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ecf7034-4ec3-4a8e-92cd-8c6dcc79afd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file(mode, filepath, s3, download_type, multimedia_url, multimedia_download_path):\n",
    "    if download_type == \"url\":\n",
    "        bucket_name = multimedia_url.split(\"/\")[2]\n",
    "        from_path_prefix = \"/\".join(multimedia_url.split(\"/\")[3:])\n",
    "        from_path = os.path.join(from_path_prefix, filepath)\n",
    "\n",
    "        to_path = os.path.join(multimedia_download_path, filepath)\n",
    "        to_path_dir =\"/\".join(to_path.split(\"/\")[:-1])\n",
    "\n",
    "        try:\n",
    "            Path(to_path_dir).mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket_name, from_path, to_path)\n",
    "        except Exception as e:\n",
    "            print(\"Error in copying file from {} to {}\".format(from_path, to_path))\n",
    "            print(str(e))\n",
    "            print(str(traceback.format_exc()))\n",
    "\n",
    "    else:\n",
    "        shutil.copy(multimedia_url + \"/\" + filepath, multimedia_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d29a8c51-58a8-4b0a-a949-1d6a22398333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, mode, feature_names, is_feature_download, download_type, multimedia_url, multimedia_download_path, output_path, **kwargs):\n",
    "    res_partition = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        for i in range(len(feature_names)):\n",
    "            feature_name = feature_names[i]\n",
    "            if is_feature_download[i]:\n",
    "                # io_a = time.time()\n",
    "                if download_type == \"url\":\n",
    "                    s3 = boto3.client('s3', aws_access_key_id=ACCESS_ID, aws_secret_access_key= ACCESS_KEY)\n",
    "                else:\n",
    "                    s3 = None\n",
    "                download_file(mode, row[feature_name], s3, download_type, multimedia_url, multimedia_download_path)\n",
    "                # io_b = time.time()\n",
    "                # io_time += io_b - io_a\n",
    "\n",
    "        # comp_a = time.time()\n",
    "        to_path = multimedia_download_path\n",
    "        row_id, input_tensor, output_tensor = row_preprocessing_routine(mode, row, to_path, kwargs)\n",
    "        res_partition.append([row_id, input_tensor, output_tensor])\n",
    "    \n",
    "    result = pd.DataFrame(list(res_partition), columns=[\"id\", \"input_tensor\", \"output_tensor\"])\n",
    "    df_size_mb =  result.memory_usage(index=True, deep=True).sum() / (1024.0 * 1024.0)\n",
    "    print(\"{} -- Size of data saved : {} MB\".format(str(mode), str(df_size_mb)))\n",
    "    print(\"{} -- Number of rows in the Worker dataframe: {}\".format(str(mode),str(len(result.index))))\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    path = os.path.join(output_path, str(mode) + \"_\" + \"data\" + \".pkl\")\n",
    "    result.to_pickle(path)\n",
    "    del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction=0.3\n",
    "is_feature_download = [False, True, False, False, False, False]\n",
    "feature_names = [\"id\", \"file_name\", \"height\", \"width\", \"captions\", \"date_captured\"]\n",
    "vocab_threshold = 5\n",
    "train_vocab = Vocabulary(vocab_threshold, annotations_file=train_annotations_path, vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48f565-c591-47c1-8cf8-2ac20568ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_metadata()\n",
    "\n",
    "train_meta_df, largest_caption_len_train = get_metadata(train_annotations_path, mode=\"train\")\n",
    "val_meta_df, largest_caption_len_val = get_metadata(valid_annotations_path, mode=\"val\")\n",
    "test_meta_df, largest_caption_len_test = get_metadata(test_annotations_path, mode=\"test\")\n",
    "\n",
    "train_df_path = ROOT_PATH + \"train_metadata.csv\"\n",
    "valid_df_path = ROOT_PATH + \"valid_metadata.csv\"\n",
    "test_df_path = ROOT_PATH + \"test_metadata.csv\"\n",
    "\n",
    "train_meta_df.to_csv(train_df_path, index=False)\n",
    "val_meta_df.to_csv(valid_df_path, index=False)\n",
    "test_meta_df.to_csv(test_df_path, index=False)\n",
    "\n",
    "\n",
    "train_df = load_data(train_df_path, fraction)\n",
    "valid_df = load_data(valid_df_path, fraction)\n",
    "test_df = load_data(test_df_path, fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec0490d7-b154-40d4-8cbd-b9df8f31a2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/ubuntu/data/train_metadata.csv metadata to Pandas\n",
      "Loaded /home/ubuntu/data/valid_metadata.csv metadata to Pandas\n",
      "Loaded /home/ubuntu/data/test_metadata.csv metadata to Pandas\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(train_df, \"train\", feature_names, is_feature_download, \"url\", train_multimedia_url, train_download_path, train_output_path, max_caption_len=largest_caption_len_train, vocab=train_vocab)\n",
    "preprocess_data(valid_df, \"valid\", feature_names, is_feature_download, \"url\", valid_multimedia_url, valid_download_path, valid_output_path, max_caption_len=largest_caption_len_val, vocab=train_vocab)\n",
    "preprocess_data(test_df, \"test\", feature_names, is_feature_download, \"url\", test_multimedia_url, test_download_path, test_output_path, max_caption_len=largest_caption_len_test, vocab=train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0d95c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8c3dd2b-a994-4c18-a83f-ea78a5a9c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class GeneralPytorchDataset(IterableDataset):\n",
    "    def __init__(self, mode, pickled_data_path):\n",
    "        self.remaining_files = glob.glob(os.path.join(pickled_data_path, \"*.pkl\"))\n",
    "        self.completed_files = []\n",
    "        self.local_index = 0\n",
    "        self.current_df = None\n",
    "        self.mode = mode\n",
    "\n",
    "    def __iter__(self):\n",
    "        # initialize first file\n",
    "        \n",
    "        f = self.remaining_files.pop(0)\n",
    "        self.completed_files.append(f)\n",
    "        self.local_index = 0\n",
    "        self.current_df = pd.read_pickle(f)\n",
    "        self.current_df = self.current_df.reset_index(drop=True)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # load next and remove current dataframe if current index is more than the size of the current dataframe\n",
    "        \n",
    "        self.local_index += 1\n",
    "        \n",
    "        if self.local_index >= len(self.current_df.index):\n",
    "            # completed all files\n",
    "            if not self.remaining_files:\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                f = self.remaining_files.pop(0)\n",
    "                self.completed_files.append(f)\n",
    "                self.local_index = 0\n",
    "                self.current_df = pd.read_pickle(f)\n",
    "                self.current_df = self.current_df.reset_index(drop=True)\n",
    "                gc.collect()\n",
    "        \n",
    "        input_tensor = self.current_df[\"input_tensor\"][self.local_index]\n",
    "        output_tensor = self.current_df[\"output_tensor\"][self.local_index]\n",
    "        row_id =  self.current_df[\"id\"][self.local_index]\n",
    "\n",
    "        return input_tensor, output_tensor, row_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        # parse through all files to determine total number of examples\n",
    "        \n",
    "        total_length = 0\n",
    "        all_files = self.remaining_files + self.completed_files\n",
    "        \n",
    "        for f in all_files:\n",
    "            df = pd.read_pickle(f)\n",
    "            total_length += len(df.index)\n",
    "        \n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b6d20f5-66b4-442a-9cc6-a30f3f528e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data_path, valid_data_path, test_data_path):\n",
    "    trainset =  GeneralPytorchDataset(\"train\", train_data_path)\n",
    "    validset = GeneralPytorchDataset(\"valid\", valid_data_path)\n",
    "    testset = GeneralPytorchDataset(\"test\", test_data_path)\n",
    "    return trainset, validset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda5e6a",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "eac52650-864c-4d0f-817c-3b06426b2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coco(val_annotations_path, train_data_path, valid_data_path, test_data_path, vocab, checkpoint_dir, config):\n",
    "    import os\n",
    "    import time\n",
    "    import sys\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    from coco_proc.model import EncoderCNN, DecoderRNN\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    \n",
    "    def get_actual_annotations(annotations_path):\n",
    "        data_json = None\n",
    "        with open(annotations_path) as f:\n",
    "            data_json = json.load(f)\n",
    "        annotations = {}\n",
    "        annotations_list = data_json['annotations']\n",
    "        for i in annotations_list:\n",
    "            if not i[\"image_id\"] in annotations:\n",
    "                annotations[i[\"image_id\"]] = []\n",
    "            annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "        return annotations\n",
    "    \n",
    "    def word_list(word_idx_list, vocab):\n",
    "        word_list = []\n",
    "        for i in range(len(word_idx_list)):\n",
    "            vocab_id = word_idx_list[i]\n",
    "            word = vocab.idx2word[vocab_id]\n",
    "            if word == vocab.end_word:\n",
    "                break\n",
    "            if word != vocab.start_word:\n",
    "                word_list.append(word)\n",
    "        return word_list\n",
    "\n",
    "    annotations_valid = get_actual_annotations(val_annotations_path)\n",
    "    # Initialize smoothing function\n",
    "    smoothing = SmoothingFunction()\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    embed_size = config[\"embed_size\"]\n",
    "    hidden_size = config[\"hidden_size\"]\n",
    "    \n",
    "    encoder = EncoderCNN(embed_size)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            encoder = nn.DataParallel(encoder)\n",
    "            decoder = nn.DataParallel(decoder)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Specify the learnable parameters of the model\n",
    "    params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "    optimizer = torch.optim.Adam(params=params, lr=learning_rate)\n",
    "#     if checkpoint_dir:\n",
    "#         checkpoint = torch.load(\n",
    "#             os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        \n",
    "#         encoder.load_state_dict(checkpoint['encoder'])\n",
    "#         decoder.load_state_dict(checkpoint['decoder'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "    trainset, validset, testset = load_data(train_data_path, valid_data_path, test_data_path)\n",
    "    print(\"trainset length:\" + str(len(trainset)))\n",
    "    print(\"valset length:\" + str(len(validset)))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=int(batch_size),\n",
    "        num_workers=8, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=int(batch_size),\n",
    "        num_workers=8, drop_last=True) # num workers here given explicitly\n",
    "    \n",
    "    print(config)\n",
    "    for epoch in range(1):\n",
    "        total_train_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        val_steps = 0\n",
    "        total_bleu_4 = 0.0\n",
    "\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            images, captions = batch[0].to(device), batch[1].to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            epoch_steps += 1\n",
    "#             print(\"[%d, %d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "#                                                 total_train_loss / epoch_steps))\n",
    "    \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            print(\"started validation\")\n",
    "            # Obtain the batch\n",
    "            for batch in val_loader:\n",
    "                images, captions, row_ids = batch[0].to(device), batch[1].to(device), batch[2]\n",
    "\n",
    "                # Pass the inputs through the CNN-RNN model\n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions).to(\"cpu\")\n",
    "\n",
    "                # move outputs back to CPU\n",
    "                captions = captions.to(\"cpu\")\n",
    "\n",
    "                # Calculate the total Bleu-4 score for the batch\n",
    "                batch_bleu_4 = 0.0\n",
    "                # Iterate over outputs. Note: outputs[i] is a caption in the batch\n",
    "                # outputs[i, j, k] contains the model's predicted score i.e. how \n",
    "                # likely the j-th token in the i-th caption in the batch is the \n",
    "                # k-th token in the vocabulary.\n",
    "                for i in range(len(outputs)):\n",
    "                    predicted_ids = []\n",
    "                    for scores in outputs[i]:\n",
    "                        # Find the index of the token that has the max score\n",
    "                        predicted_ids.append(scores.argmax().item())\n",
    "                    # Convert word ids to actual words\n",
    "                    predicted_word_list = word_list(predicted_ids, vocab)\n",
    "                    caption_word_list = word_list(captions[i].numpy(), vocab)\n",
    "                    # Calculate Bleu-4 score and append it to the batch_bleu_4 list\n",
    "                    tokenized_references = [nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "                                           for caption in annotations_valid[row_ids[i].item()]]\n",
    "                    batch_bleu_4 += sentence_bleu(tokenized_references, \n",
    "                                                   predicted_word_list, \n",
    "                                                   smoothing_function=smoothing.method1)\n",
    "                total_bleu_4 += batch_bleu_4 / len(outputs)\n",
    "                val_steps += 1\n",
    "                # Calculate the batch loss\n",
    "                loss = criterion(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                \n",
    "                # Print validation statistics (on same line)\n",
    "                sys.stdout.flush()\n",
    "#                 print(\"Validation Stats: {}\".format(stats))\n",
    "                \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\"encoder\": encoder.state_dict(),\n",
    "                \"decoder\": decoder.state_dict(),\n",
    "                \"optimizer\" : optimizer.state_dict()\n",
    "               }, path)\n",
    "\n",
    "        tune.report(loss=(total_val_loss / val_steps), bleu=total_bleu_4 / val_steps)\n",
    "    print(\"Finished training\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "adc61f83-10b1-4755-94c8-5494bf5bd67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = train_output_path\n",
    "valid_data_path = valid_output_path\n",
    "test_data_path = test_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "61a53cd4-6344-44b6-b8c3-7551f4248a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_epochs = 1\n",
    "# config = {\n",
    "#         \"learning_rate\": tune.grid_search([1e-2, 1e-3]),\n",
    "#         \"embed_size\": tune.grid_search([256, 512]),\n",
    "#         \"hidden_size\": tune.grid_search([256, 512]),\n",
    "#         \"batch_size\": tune.grid_search([2, 4, 8, 16])\n",
    "#     }\n",
    "config = {\n",
    "        \"learning_rate\": tune.grid_search([1e-3]),\n",
    "        \"embed_size\": tune.grid_search([256,512]),\n",
    "        \"hidden_size\": tune.grid_search([256]),\n",
    "        \"batch_size\": tune.grid_search([2])\n",
    "    }\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "\n",
    "# checkpoint_dir = \".\"\n",
    "reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"bleu\", \"training_iteration\"])\n",
    "result = tune.run(\n",
    "    partial(train_coco, valid_annotations_path, train_data_path, valid_data_path, test_data_path, train_vocab, checkpoints_dir),\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 0},\n",
    "    config=config,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a992b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "# implement this for hyperparameter tuning (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177900c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
