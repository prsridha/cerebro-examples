{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2226db6",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040798e3",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "### <font color='blue'>Cerebro Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d167244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cerebro.etl.etl_spec import ETLSpec\n",
    "from cerebro.experiment import Experiment\n",
    "from cerebro.mop.sub_epoch_spec import SubEpochSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6bc0f",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "### <font color='blue'> Initialize Data Preprocessing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b125186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoETLSpec(ETLSpec):\n",
    "    def __init__(self):\n",
    "        from torchvision import transforms\n",
    "        from coco_proc.vocabulary import Vocabulary\n",
    "\n",
    "        vocab_threshold = 5\n",
    "        self.max_caption_len = 55\n",
    "        self.miscellaneous_path = \"/data/cerebro_data_storage/miscellaneous\"\n",
    "        self.is_feature_download = [False, True, False, False, False, False]\n",
    "        annotations_file = self.miscellaneous_path + \"/captions_train2017.json\"\n",
    "        self.train_vocab = Vocabulary(vocab_threshold, annotations_file=annotations_file, vocab_from_file=False)\n",
    "        \n",
    "        self.train_img_transform = transforms.Compose([ \n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                    (0.229, 0.224, 0.225))])\n",
    "        \n",
    "        self.valid_img_transform = transforms.Compose([ \n",
    "                transforms.Resize(256),                          \n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),                           \n",
    "                transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                                    (0.229, 0.224, 0.225))])\n",
    "\n",
    "    def initialize_worker(self):\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def row_preprocessor(self, row, mode, object_dir):\n",
    "        import nltk\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "\n",
    "        vocab = self.train_vocab    \n",
    "        max_caption_len = self.max_caption_len\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            img_transform = self.train_img_transform\n",
    "        else:\n",
    "            img_transform = self.valid_img_transform\n",
    "        \n",
    "        # reading input features and converting to tensor\n",
    "        input_image_path = object_dir + \"/\" + str(row[\"file_name\"])\n",
    "        image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        image_tensor = img_transform(image)\n",
    "        \n",
    "        # reading output features and converting to tensor\n",
    "        output_caption = row[\"captions\"]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(output_caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab(vocab.start_word))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab(vocab.end_word))\n",
    "        \n",
    "        # padding\n",
    "        nremaining = max_caption_len - len(tokens)\n",
    "        if nremaining > 0:\n",
    "            for i in range(nremaining):\n",
    "                caption.append(vocab(vocab.end_word))\n",
    "        \n",
    "        caption_tensor = torch.Tensor(caption).long()\n",
    "\n",
    "        return image_tensor, caption_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde773dc",
   "metadata": {},
   "source": [
    "### <font color='blue'> Define the model training and validation functions </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8b676-a3cb-4226-b9d9-f6b48c9174d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainingSpec(SubEpochSpec):\n",
    "    def __init__(self):\n",
    "        from coco_proc.vocabulary import Vocabulary\n",
    "        \n",
    "        vocab_threshold = 5\n",
    "        self.miscellaneous_path = \"/data/cerebro_data_storage/miscellaneous\"\n",
    "        annotations_file = self.miscellaneous_path + \"/captions_train2017.json\"\n",
    "        self.train_vocab = Vocabulary(vocab_threshold, annotations_file=annotations_file, vocab_from_file=False)\n",
    "\n",
    "    def get_actual_annotations(self, annotations_path):\n",
    "        import json\n",
    "        data_json = None\n",
    "        with open(annotations_path) as f:\n",
    "            data_json = json.load(f)\n",
    "        annotations = {}\n",
    "        annotations_list = data_json['annotations']\n",
    "        for i in annotations_list:\n",
    "            if not i[\"image_id\"] in annotations:\n",
    "                annotations[i[\"image_id\"]] = []\n",
    "            annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "        return annotations\n",
    "\n",
    "    def word_list(self, word_idx_list, vocab):\n",
    "        word_list = []\n",
    "        for i in range(len(word_idx_list)):\n",
    "            vocab_id = word_idx_list[i]\n",
    "            word = vocab.idx2word[vocab_id]\n",
    "            if word == vocab.end_word:\n",
    "                break\n",
    "            if word != vocab.start_word:\n",
    "                word_list.append(word)\n",
    "        return word_list\n",
    "\n",
    "    def initialize_worker(self):\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def create_model_components(self, hyperparams):\n",
    "        import torch\n",
    "        from coco_proc.model import EncoderCNN, DecoderRNN\n",
    "\n",
    "        # Obtain hyperparameters\n",
    "        vocab_size = len(self.train_vocab)\n",
    "        learning_rate = hyperparams[\"learning_rate\"]\n",
    "        embed_size = hyperparams[\"embed_size\"]\n",
    "        hidden_size = hyperparams[\"hidden_size\"]\n",
    "\n",
    "        # Create the models\n",
    "        encoder = EncoderCNN(embed_size)\n",
    "        decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "        models = [encoder, decoder]\n",
    "\n",
    "        # Specify the learnable parameters of the model\n",
    "        params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "        \n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(params=params, lr=learning_rate)\n",
    "\n",
    "        return models, optimizer\n",
    "\n",
    "    def train(self, models, optimizer, checkpoint, dataloader, hyperparams, input_device, output_device):\n",
    "        import math\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import torch.nn as nn\n",
    "\n",
    "        # get hyperparams\n",
    "        vocab_size = len(self.train_vocab)\n",
    "        batch_size = hyperparams[\"batch_size\"]\n",
    "\n",
    "        # Define the loss function\n",
    "        criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "        # get models\n",
    "        encoder, decoder = models\n",
    "\n",
    "        states = checkpoint(\"load\")\n",
    "        if states:\n",
    "            encoder.load_state_dict(states[\"encoder\"])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Start time for every 100 steps\n",
    "        i_step = 0\n",
    "        total_loss = 0.0\n",
    "        subepoch_total_step = math.ceil(len(dataloader.dataset) / batch_size)\n",
    "        train_metrics = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            images, captions = batch[0].to(input_device), batch[1].to(output_device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            stats_dict = {\n",
    "                \"loss\": loss.item(),\n",
    "                \"perplexity\": np.exp(loss.item())\n",
    "            }\n",
    "\n",
    "            stats = \"Train step [%d/%d], Loss: %.4f, Perplexity: %5.4f\" \\\n",
    "                        % (i_step, subepoch_total_step, loss.item(), np.exp(loss.item()))\n",
    "            print(\"\\r\" + stats, end=\"\")\n",
    "\n",
    "            train_metrics.append(stats_dict)\n",
    "\n",
    "            i_step += 1\n",
    "\n",
    "        checkpoint(\"save\", {\"encoder\": encoder.state_dict(),\n",
    "                    \"decoder\": decoder.state_dict(),\n",
    "                    \"optimizer\" : optimizer.state_dict()\n",
    "                   })\n",
    "\n",
    "        return train_metrics\n",
    "    \n",
    "    def test(self, models, checkpoint, dataloader, hyperparams, input_device, output_device):\n",
    "        import math\n",
    "        import nltk\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import torch.nn as nn\n",
    "        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "        # get hyperparams\n",
    "        vocab = self.train_vocab\n",
    "        batch_size = hyperparams[\"batch_size\"]\n",
    "\n",
    "        val_annotations_path = self.miscellaneous_path + \"/captions_val2017.json\"\n",
    "        annotations_valid = self.get_actual_annotations(val_annotations_path)\n",
    "\n",
    "        # get models\n",
    "        encoder, decoder = models\n",
    "\n",
    "        # Define the loss function\n",
    "        criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "        states = checkpoint(\"load\")\n",
    "        if states:\n",
    "            encoder.load_state_dict(states[\"encoder\"])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "        # Switch to validation mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Initialize smoothing function\n",
    "        smoothing = SmoothingFunction()\n",
    "\n",
    "        # Keep track of validation loss and Bleu-4 score\n",
    "        total_loss = start_loss\n",
    "        total_bleu_4 = start_bleu\n",
    "\n",
    "        start_loss=0.0\n",
    "        start_bleu = 0.0\n",
    "        subepoch_total_step = math.ceil(len(dataloader.dataset) / batch_size)\n",
    "        \n",
    "        start_step=1\n",
    "        with torch.no_grad():\n",
    "            # Obtain the batch\n",
    "            for batch in dataloader:\n",
    "                images, captions, row_ids = batch[0].to(input_device), batch[1].to(output_device), batch[2]\n",
    "\n",
    "                # Pass the inputs through the CNN-RNN model\n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions).to(\"cpu\")\n",
    "\n",
    "                # move outputs back to CPU\n",
    "                captions = captions.to(\"cpu\")\n",
    "\n",
    "                # Calculate the total Bleu-4 score for the batch\n",
    "                batch_bleu_4 = 0.0\n",
    "                for i in range(len(outputs)):\n",
    "                    predicted_ids = []\n",
    "                    for scores in outputs[i]:\n",
    "                        predicted_ids.append(scores.argmax().item())\n",
    "                    predicted_word_list = self.word_list(predicted_ids, vocab)\n",
    "                    # caption_word_list = self.word_list(captions[i].numpy(), vocab)\n",
    "\n",
    "                    tokenized_references = [nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "                                           for caption in annotations_valid[row_ids[i].item()]]\n",
    "                    batch_bleu_4 += sentence_bleu(tokenized_references, \n",
    "                                                   predicted_word_list, \n",
    "                                                   smoothing_function=smoothing.method1)\n",
    "                total_bleu_4 += batch_bleu_4 / len(outputs)\n",
    "\n",
    "                # Calculate the batch loss\n",
    "                loss = criterion(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Get validation statistics\n",
    "                stats = \"Val step [%d/%d], Loss: %.4f, Perplexity: %5.4f, Batch Bleu-4: %.4f\" \\\n",
    "                        % (start_step, subepoch_total_step,\n",
    "                           loss.item(), np.exp(loss.item()), batch_bleu_4 / len(outputs))\n",
    "                print(\"\\r\" + stats, end=\"\")\n",
    "\n",
    "                start_step += 1\n",
    "\n",
    "            test_metrics = {\n",
    "                \"total_epoch_loss\": total_loss / subepoch_total_step,\n",
    "                \"total_bleu_4\": total_bleu_4 / subepoch_total_step\n",
    "            }\n",
    "\n",
    "            return test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b38d8-0034-42fd-98d4-a1a54e4d47c1",
   "metadata": {},
   "source": [
    "### <font color='blue'> Run Cerebro </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'embed_size': [256],\n",
    "    'hidden_size': [256],\n",
    "    'batch_size': [128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment()\n",
    "coco_etl_spec = CocoETLSpec()\n",
    "coco_training_spec = CocoTrainingSpec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943f47a-a3a1-42a6-bd77-f9db47e5365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run_etl(coco_etl_spec, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76d69-287c-428c-b95c-67389adb3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run_fit(coco_training_spec, param_grid, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b8912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5b5644f121a57b48c77344d1984be13650597feb249d6c9b181310f236f11d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
