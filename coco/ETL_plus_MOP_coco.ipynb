{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2226db6",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040798e3",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "### <font color='blue'>Cerebro Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d167244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cerebro.etl_spec import ETLSpec\n",
    "from cerebro.experiment import Experiment\n",
    "from cerebro.sub_epoch_spec import SubEpochSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6bc0f",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "### <font color='blue'> Initialize Data Preprocessing </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658ce30-f6da-474c-a8c4-eda7bbc217cf",
   "metadata": {},
   "source": [
    "##### <font color='grey'> Set up data preprocessing workers by installing necessary package dependencies. Only include one-time execution statements, such as downloading nltk's \"punkt\" library as shown below. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b125186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoETLSpec(ETLSpec):\n",
    "    def __init__(self):\n",
    "        from coco_proc.vocabulary import Vocabulary\n",
    "\n",
    "        self.miscellaneous_path = \"/data/cerebro_data_storage/miscellaneous\"\n",
    "        self.is_feature_download = [True, False, False, False]\n",
    "        \n",
    "        vocab_threshold = 5\n",
    "        annotations_file = self.miscellaneous_path + \"/captions_train2017.json\"\n",
    "        self.train_vocab = Vocabulary(vocab_threshold, annotations_file=annotations_file, vocab_from_file=False)\n",
    "        self.max_caption_len = 55\n",
    "\n",
    "    def initialize_worker(self):\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def row_preprocessor(self, row, mode, object_dir):\n",
    "        \"\"\"\n",
    "        Convert a given dataset row to tensor format (suitable for training)\n",
    "        Data processing is a data parallel map operation. So, the same row_preprocessing_routine() will\n",
    "        be called on every row of the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : pandas.core.series.Series\n",
    "            metadata pandas dataframe row\n",
    "        mode : str\n",
    "            train/valid/test\n",
    "        object_dir : str\n",
    "            Path where all multimedia files will be stored on a node\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        id : str\n",
    "            uniquely identifying this row\n",
    "        input_tensor : torch.Tensor\n",
    "            All the input features should be combined to form a single input tensor\n",
    "        output_tensor : torch.Tensor\n",
    "            All the output features should be combined to form a single output tensor\n",
    "\n",
    "        \"\"\"\n",
    "        import nltk\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "        from torchvision import transforms\n",
    "            \n",
    "        max_caption_len = self.max_caption_len\n",
    "        vocab = self.train_vocab\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            img_transform = transforms.Compose([ \n",
    "                transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "                transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "                transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "                transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                    (0.229, 0.224, 0.225))])\n",
    "        else:\n",
    "            img_transform = transforms.Compose([ \n",
    "                transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "                transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "                transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                    (0.229, 0.224, 0.225))])\n",
    "        \n",
    "        # reading input features and converting to tensor\n",
    "        input_image_path = object_dir + \"/\" + str(row[\"file_name\"])\n",
    "        image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        image_tensor = img_transform(image)\n",
    "        \n",
    "        # reading output features and converting to tensor\n",
    "        output_caption = row[\"captions\"]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(output_caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab(vocab.start_word))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab(vocab.end_word))\n",
    "        \n",
    "        # padding\n",
    "        nremaining = max_caption_len - len(tokens)\n",
    "        if nremaining > 0:\n",
    "            for i in range(nremaining):\n",
    "                caption.append(vocab(vocab.end_word))\n",
    "        \n",
    "        caption_tensor = torch.Tensor(caption).long()\n",
    "\n",
    "        return image_tensor, caption_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde773dc",
   "metadata": {},
   "source": [
    "### <font color='blue'> Define the model training and validation functions </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8b676-a3cb-4226-b9d9-f6b48c9174d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainingSpec(SubEpochSpec):\n",
    "    def __init__(self):\n",
    "        self.miscellaneous_path = \"/data/cerebro_data_storage/miscellaneous\"\n",
    "\n",
    "        from coco_proc.vocabulary import Vocabulary\n",
    "        vocab_threshold = 5\n",
    "        annotations_file = self.miscellaneous_path + \"/captions_train2017.json\"\n",
    "        self.train_vocab = Vocabulary(vocab_threshold, annotations_file=annotations_file, vocab_from_file=False)\n",
    "\n",
    "    def initialize_worker(self):\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def train(self, model_file, train_dataset, config, logging, device):\n",
    "        \"\"\"\n",
    "        User has to define a train function which will load a model configuration and perform training.\n",
    "\n",
    "        Parameters\n",
    "        ----------       \n",
    "        model_file : str\n",
    "            Load model file if it exists else create model and save model after every sub epoch\n",
    "        train_dataset : torch.utils.data.IterableDataset\n",
    "            Dataset is in the form of Pytorch dataset which can be processed by Pytorch Dataloaders\n",
    "        config : dict\n",
    "            Dictionary of hyperparameters (key is hyperparam name and value is its value) for a model training setup\n",
    "        logging : Logger\n",
    "            Logger for logging any form of training information\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ml_metrics : dict\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO : explain ml_metrics (simplify it?)\n",
    "        import os\n",
    "        import time\n",
    "        import sys\n",
    "        import math\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import tensorflow as tf\n",
    "        from torch.utils.data import DataLoader\n",
    "        from coco_proc.model import EncoderCNN, DecoderRNN\n",
    "\n",
    "        \n",
    "        vocab = self.train_vocab\n",
    "        vocab_size = len(vocab)\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        embed_size = config[\"embed_size\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "                    train_dataset, batch_size)\n",
    "        \n",
    "        encoder = EncoderCNN(embed_size)\n",
    "        decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "        # Define the loss function\n",
    "        criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "        # Specify the learnable parameters of the model\n",
    "        params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "        # move model to GPU\n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(params=params, lr=learning_rate)\n",
    "\n",
    "        if os.path.isfile(model_file):\n",
    "            checkpoint = torch.load(model_file)\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Start time for every 100 steps\n",
    "        start_train_time = time.time()\n",
    "        i_step = 0\n",
    "        total_loss = 0.0\n",
    "        subepoch_total_step = math.ceil(len(train_loader.dataset) / batch_size)\n",
    "        train_results = {\n",
    "            \"stats\": [],\n",
    "            \"additive_metrics\": {}\n",
    "        }\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images, captions = batch[0].to(device), batch[1].to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            stats_dict = {\n",
    "                \"loss\": loss.item(),\n",
    "                \"perplexity\": np.exp(loss.item())\n",
    "            }\n",
    "\n",
    "            stats = \"Train step [%d/%d], %ds, Loss: %.4f, Perplexity: %5.4f\" \\\n",
    "                        % (i_step, subepoch_total_step, time.time() - start_train_time,\n",
    "                           loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            print(\"\\r\" + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            logging.info(\"Training Stats: {}\".format(stats))\n",
    "\n",
    "            train_results[\"stats\"].append(stats_dict)\n",
    "\n",
    "            i_step += 1\n",
    "\n",
    "        torch.save({\"encoder\": encoder.state_dict(),\n",
    "                    \"decoder\": decoder.state_dict(),\n",
    "                    \"optimizer\" : optimizer.state_dict(),\n",
    "                    \"total_loss\": total_loss\n",
    "                   }, model_file)\n",
    "\n",
    "        \n",
    "        train_results[\"additive_metrics\"][\"sub_epoch_total_loss\"] = total_loss\n",
    "\n",
    "        return train_results\n",
    "    \n",
    "    def test(self, model_file, test_dataset, config, logging, device):\n",
    "        \"\"\"\n",
    "        User has to define a validation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_file : str\n",
    "            Load model file for validation\n",
    "        test_dataset : torch.utils.data.IterableDataset\n",
    "            Dataset is in the form of Pytorch dataset which can be processed by Pytorch Dataloaders\n",
    "        config : dict\n",
    "            Dictionary of hyperparameters (key is hyperparam name and value is its value)\n",
    "        logging : Logger\n",
    "            Logger for logging any form of training information\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ml_metrics : dict\n",
    "\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import time\n",
    "        import sys\n",
    "        import math\n",
    "        import numpy as np\n",
    "        import nltk\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import tensorflow as tf\n",
    "        from torch.utils.data import DataLoader\n",
    "        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "        from coco_proc.model import EncoderCNN, DecoderRNN\n",
    "\n",
    "\n",
    "        \"\"\"Validate the model for one epoch using the provided parameters. \n",
    "        Return the epoch's average validation loss and Bleu-4 score.\"\"\"\n",
    "\n",
    "        def get_actual_annotations(annotations_path):\n",
    "            data_json = None\n",
    "            with open(annotations_path) as f:\n",
    "                data_json = json.load(f)\n",
    "            annotations = {}\n",
    "            annotations_list = data_json['annotations']\n",
    "            for i in annotations_list:\n",
    "                if not i[\"image_id\"] in annotations:\n",
    "                    annotations[i[\"image_id\"]] = []\n",
    "                annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "            return annotations\n",
    "\n",
    "        def word_list(word_idx_list, vocab):\n",
    "            word_list = []\n",
    "            for i in range(len(word_idx_list)):\n",
    "                vocab_id = word_idx_list[i]\n",
    "                word = vocab.idx2word[vocab_id]\n",
    "                if word == vocab.end_word:\n",
    "                    break\n",
    "                if word != vocab.start_word:\n",
    "                    word_list.append(word)\n",
    "            return word_list\n",
    "\n",
    "\n",
    "        # initialize device for GPU\n",
    "        val_annotations_path = self.miscellaneous_path + \"/captions_val2017.json\"\n",
    "\n",
    "        annotations_valid = get_actual_annotations(val_annotations_path)\n",
    "\n",
    "        vocab = self.train_vocab\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        embed_size = config[\"embed_size\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "                    test_dataset, batch_size)\n",
    "\n",
    "        subepoch_total_step = math.ceil(len(val_loader.dataset) / batch_size)\n",
    "        start_step=1 \n",
    "        start_loss=0.0\n",
    "        start_bleu = 0.0\n",
    "\n",
    "\n",
    "        encoder = EncoderCNN(embed_size)\n",
    "        decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "        # Define the loss function\n",
    "        criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "        # Specify the learnable parameters of the model\n",
    "        params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "        # move model to GPU\n",
    "        encoder.to(device)\n",
    "        decoder.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(params=params, lr=learning_rate)\n",
    "\n",
    "        if os.path.isfile(model_file):\n",
    "            checkpoint = torch.load(model_file)\n",
    "            encoder.load_state_dict(checkpoint['encoder'])\n",
    "            decoder.load_state_dict(checkpoint['decoder'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "        # Switch to validation mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Initialize smoothing function\n",
    "        smoothing = SmoothingFunction()\n",
    "\n",
    "        # Keep track of validation loss and Bleu-4 score\n",
    "        total_loss = start_loss\n",
    "        total_bleu_4 = start_bleu\n",
    "\n",
    "        # Start time for every 100 steps\n",
    "        start_val_time = time.time()\n",
    "        test_results = {\n",
    "            \"stats\": {}\n",
    "        }\n",
    "\n",
    "        # Disable gradient calculation because we are in inference mode\n",
    "        dc = 0\n",
    "        with torch.no_grad():\n",
    "            # Obtain the batch\n",
    "            for batch in val_loader:\n",
    "                images, captions, row_ids = batch[0].to(device), batch[1].to(device), batch[2]\n",
    "\n",
    "                # Pass the inputs through the CNN-RNN model\n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions).to(\"cpu\")\n",
    "\n",
    "                # move outputs back to CPU\n",
    "                captions = captions.to(\"cpu\")\n",
    "\n",
    "                # Calculate the total Bleu-4 score for the batch\n",
    "                batch_bleu_4 = 0.0\n",
    "                # Iterate over outputs. Note: outputs[i] is a caption in the batch\n",
    "                # outputs[i, j, k] contains the model's predicted score i.e. how \n",
    "                # likely the j-th token in the i-th caption in the batch is the \n",
    "                # k-th token in the vocabulary.\n",
    "                for i in range(len(outputs)):\n",
    "                    predicted_ids = []\n",
    "                    for scores in outputs[i]:\n",
    "                        # Find the index of the token that has the max score\n",
    "                        predicted_ids.append(scores.argmax().item())\n",
    "                    # Convert word ids to actual words\n",
    "                    predicted_word_list = word_list(predicted_ids, vocab)\n",
    "                    caption_word_list = word_list(captions[i].numpy(), vocab)\n",
    "\n",
    "                    # Calculate Bleu-4 score and append it to the batch_bleu_4 list\n",
    "                    tokenized_references = [nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "                                           for caption in annotations_valid[row_ids[i].item()]]\n",
    "                    batch_bleu_4 += sentence_bleu(tokenized_references, \n",
    "                                                   predicted_word_list, \n",
    "                                                   smoothing_function=smoothing.method1)\n",
    "                total_bleu_4 += batch_bleu_4 / len(outputs)\n",
    "\n",
    "                # Calculate the batch loss\n",
    "                loss = criterion(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Get validation statistics\n",
    "                stats = \"Val step [%d/%d], Loss: %.4f, Perplexity: %5.4f, Batch Bleu-4: %.4f\" \\\n",
    "                        % (start_step, subepoch_total_step,\n",
    "                           loss.item(), np.exp(loss.item()), batch_bleu_4 / len(outputs))\n",
    "                # Print validation statistics (on same line)\n",
    "                print(\"\\r\" + stats, end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                logging.info(\"Validation Stats: {}\".format(stats))\n",
    "\n",
    "                start_step += 1\n",
    "\n",
    "            test_results[\"stats\"][\"total_loss\"] = total_loss / subepoch_total_step\n",
    "            test_results[\"stats\"][\"total_bleu_4\"] = total_bleu_4 / subepoch_total_step\n",
    "            return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b38d8-0034-42fd-98d4-a1a54e4d47c1",
   "metadata": {},
   "source": [
    "### <font color='blue'> Run Cerebro </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'embed_size': [256],\n",
    "    'hidden_size': [256],\n",
    "    'batch_size': [128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment()\n",
    "etl_spec = CocoETLSpec()\n",
    "sub_epoch_spec = CocoTrainingSpec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943f47a-a3a1-42a6-bd77-f9db47e5365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run_etl(etl_spec, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76d69-287c-428c-b95c-67389adb3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run_fit(sub_epoch_spec, param_grid, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5b5644f121a57b48c77344d1984be13650597feb249d6c9b181310f236f11d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
